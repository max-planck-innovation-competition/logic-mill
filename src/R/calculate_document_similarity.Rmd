---
title: "Calculate Document Similarity API"
author: "Logic Mill"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
---

This API encodes a set of documents and directly computes a similarity matrix (cosine, l1, or l2) between all pairs, returning the similarity scores for further analysis.

## Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

# Set working directory
if (requireNamespace("rstudioapi", quietly = TRUE) && rstudioapi::isAvailable()) {
  setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
}

# Load environment variables
if (file.exists(".env")) {
  readRenviron(".env")
}
API_KEY <- paste('Bearer', Sys.getenv("API_KEY"))
```

```{r message=FALSE}
library(httr)
library(jsonlite)
library(ghql)
library(dplyr)
library(ggplot2)

# API URL and headers
URL <- 'https://api.logic-mill.net/api/v1/graphql/'

# Set up GraphQL client
conn <- GraphqlClient$new(
  url = URL,
  headers = list(Authorization = API_KEY)
)

# Choose model to encode the document: patspecter, specter2 or paecter
DEFAULT_MODEL <- "patspecter"

# Choose the similarity metric: cosine, l1, l2
SIMILARITY_METRIC <- "cosine"
```

## Define Documents

```{r}
# Define documents to compare
documents <- list(
  list(id = "10.1007/s41060-024-00546-5", title = "Neural lasso", 
       abstract = "In recent years, there has been a growing interest in establishing bridges between statistics and neural networks. This article focuses on the adaptation of the widely used lasso algorithm within the context of neural networks. To accomplish this, the network configuration is first designed. After that, in order to estimate the network weights, three optimization algorithms are considered. The first one, called standard neural lasso, employs the conventional procedure for training neural networks. The second optimization algorithm, termed restricted neural lasso, mimics traditional lasso to establish a connection between statistics and machine learning. Finally, a third optimization algorithm, called voting neural lasso was developed."),
  list(id = "10.1007/s00521-025-11180-y", title = "Virtual neural networks", 
       abstract = "A new concept, termed virtual neural networks, is introduced, where the count of trainable parameters is kept constant, and scalability is attained purely through computational resources. This concept is an abstract framework that can be realized using any standard convolutional neural network. It merges siamese neural networks with a deep ensemble technique by generating numerous virtual models that share weights derived from a small set of physical models."),
  list(id = "10.1007/s44258-024-00042-2", title = "Towards NeuroAI", 
       abstract = "Throughout history, the development of artificial intelligence, especially artificial neural networks, has been continuously influenced by a deeper understanding of the brain. This influence includes the development of the neocognitron, considered a precursor to convolutional neural networks. The emerging field of NeuroAI posits that leveraging neuroscience knowledge could significantly advance AI by imbuing networks with enhanced capabilities."),
  list(id = "10.1007/s10851-024-01171-4", title = "Riesz Networks", 
       abstract = "Scale invariance of an algorithm refers to its ability to treat objects equally independently of their size. For neural networks, scale invariance is typically achieved by data augmentation. However, when presented with a scale far outside the range covered by the training set, neural networks may fail to generalize. Here, we introduce the Riesz network, a novel scale-invariant neural network."),
  list(id = "10.1038/s41377-024-01590-3", title = "Optical neural networks", 
       abstract = "Artificial intelligence has prevailed in all trades and professions due to the assistance of big data resources, advanced algorithms, and high-performance electronic hardware. However, conventional computing hardware is inefficient at implementing complex tasks. In recent years, optical neural networks (ONNs) have made a range of research progress in optical computing due to advantages such as sub-nanosecond latency, low heat dissipation, and high parallelism.")
)
```

## Calculate Similarity Matrix

The `encodeDocumentAndSimilarityCalculation` endpoint encodes the documents and computes a similarity matrix in a single API call.

```{r}
# Build GraphQL query
query <- 'query encodeDocumentAndSimilarityCalculation($data: [EncodeObject], $similarityMetric: similarityMetric, $model: String!) {
  encodeDocumentAndSimilarityCalculation(
    data: $data,
    similarityMetric: $similarityMetric,
    model: $model
  ) {
    similarities
    xs {
      id
    }
    ys {
      id
    }
  }
}'

# Build variables for the query
variables <- list(
  model = DEFAULT_MODEL,
  similarityMetric = SIMILARITY_METRIC,
  data = lapply(documents, function(doc) {
    list(
      id = doc$id,
      parts = list(
        list(key = "title", value = doc$title),
        list(key = "abstract", value = doc$abstract)
      )
    )
  })
)

# Execute query
new <- Query$new()$query('link', query)
res <- conn$exec(new$link, variables = variables) %>%
    fromJSON(flatten = FALSE)
```

## View Results

```{r}
# Extract similarity data
result <- res$data$encodeDocumentAndSimilarityCalculation

# Get similarity matrix
sim_matrix <- matrix(unlist(result$similarities), 
                     nrow = length(documents), 
                     byrow = TRUE)

# Add row/column names from API response
colnames(sim_matrix) <- sapply(documents, function(x) x$title)
rownames(sim_matrix) <- sapply(documents, function(x) x$title)

cat("Documents Similarity Matrix:\n")
print(round(sim_matrix, 3))
```

## Heatmap for Similarity Matrix

To better understand the relationships between the encoded documents, we can visualize the similarity matrix as a heatmap. This graphical representation makes it easy to identify which documents are most similar to each other based on the chosen similarity metric.

```{r fig.width=10, fig.height=8}
# Convert to long format for ggplot2
sim_df <- as.data.frame(as.table(sim_matrix))
names(sim_df) <- c("Doc1", "Doc2", "Similarity")

# Create heatmap
ggplot(sim_df, aes(x = Doc1, y = Doc2, fill = Similarity)) +
  geom_tile() +
  geom_text(aes(label = round(Similarity, 2)), color = "white", size = 3) +
  scale_fill_viridis_c() +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1, size = 8),
        axis.text.y = element_text(size = 8)) +
  labs(title = paste("Document Similarity Heatmap (", SIMILARITY_METRIC, ")", sep = ""),
       x = "", y = "")
```

## Summary Statistics

```{r}
# Extract off-diagonal elements (exclude self-similarity)
off_diag <- sim_matrix[row(sim_matrix) != col(sim_matrix)]

cat("Similarity Statistics (excluding self-similarity):\n")
cat("  Min:", round(min(off_diag), 3), "\n")
cat("  Max:", round(max(off_diag), 3), "\n")
cat("  Mean:", round(mean(off_diag), 3), "\n")
cat("  Median:", round(median(off_diag), 3), "\n")
```
